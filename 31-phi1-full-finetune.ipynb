{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa45d2-01fa-4118-81f1-aaca7a53edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d0497-7216-4760-89f9-ff9398f624ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Hyper-parameters\n",
    "###################\n",
    "args = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"phi1-ai-abstracts\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100000000000000,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "}\n",
    "\n",
    "training_args = TrainingArguments(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbd904-215d-4444-a2ad-74decb3e8d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Model Loading\n",
    "################\n",
    "checkpoint_path = \"microsoft/Phi-1.5\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attention support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f0aab7-8c77-47c6-a556-360b22859b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "phi3_prompt = \"\"\"<|system|>\n",
    "You are an educated researcher and always answer in correct scientific terms.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Write an abstract for \"{}\"\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "Abstract:\n",
    "\n",
    "{}\n",
    "<|end|>\"\"\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    titles    = examples[\"title\"]\n",
    "    abstracts = examples[\"abstract\"]\n",
    "    texts = []\n",
    "    for title, abstract in zip(titles,  abstracts):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = phi3_prompt.format(title, abstract)\n",
    "        texts.append(text)\n",
    "\n",
    "    # first instruction should start with system prompt?\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd6f015-e76d-4a18-9731-b63c5acd7bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4071fadbb849629b6b743841318432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'formatting_prompts_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai-abstracts.jsonl.xz\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(\u001b[43mformatting_prompts_func\u001b[49m, batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'formatting_prompts_func' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"ai-abstracts.jsonl.xz\", split=\"train\")\n",
    "train_dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc881e45-cbf4-48b7-b26c-0288e6a4abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73764c-7fca-4ac8-9b77-093615f45b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106719e-9e89-417c-939b-83dcaf37da5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1856567b-9039-48a8-85d2-0cf202960c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f870b1-0635-48f0-8cfb-a22209db789d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
